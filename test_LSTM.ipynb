{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w8HMJWXTalWf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1646899282588,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"},"user_tz":480},"id":"uyKZK7pVaxsY","outputId":"c6aaf0f1-3eb8-4375-861e-0d0d0b9978c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/NNDL/Project/ARDL\n"]}],"source":["%cd /content/drive/MyDrive/Github/ARDL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOBWQymCaZhy"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","from dataset import *\n","from torch.utils.data import TensorDataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zm6cxfjDaiKl"},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n","newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25993,"status":"ok","timestamp":1646899315755,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"},"user_tz":480},"id":"qEaXs17Ha2k8","outputId":"e4a04c90-c4a2-429c-ac82-98b5891f3b90"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary size [Bag-of-words]:  151353\n","Vocabulary size [GloVe]:  400000\n"]}],"source":["emb_mat, data = create_inp_data_and_weights(newsgroups_train, newsgroups_test, glove_dim=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1646899356514,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"},"user_tz":480},"id":"1G6j81oza5k6","outputId":"7f923074-3861-4726-82a6-c1d89c2ac98d"},"outputs":[{"data":{"text/plain":["LSTM(\n","  (embedding): Embedding(151353, 50)\n","  (lstm): LSTM(50, 45, num_layers=6, batch_first=True, dropout=0.5)\n","  (fc): Linear(in_features=22500, out_features=7, bias=True)\n","  (att): Linear(in_features=22500, out_features=7, bias=True)\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from lstm import *\n","input_size, emb_size = emb_mat.shape\n","\n","model = build_random_lstm(3, 8, 32, 128, input_size, emb_size, 7, emb_mat)\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3g_JIeiab1px"},"outputs":[],"source":["learning_rate = 0.001\n","num_epochs = 15\n","batch_size = 512\n","display_step = 4\n","\n","criterion = nn.CrossEntropyLoss()  \n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZ6USeQgdQtB"},"outputs":[],"source":["from torch.autograd import Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-_hgwBldEih","outputId":"881f8dca-91ac-40bc-921e-b76c15555751"},"outputs":[{"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/NNDL/Project/ARDL/lstm.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return a, F.log_softmax(y)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/15], Step [4/22], Loss: 1.7907\n","Epoch [1/15], Step [8/22], Loss: 1.7704\n","Epoch [1/15], Step [12/22], Loss: 1.7846\n","Epoch [1/15], Step [16/22], Loss: 1.8060\n","Epoch [1/15], Step [20/22], Loss: 1.7627\n","Epoch [2/15], Step [4/22], Loss: 1.7487\n","Epoch [2/15], Step [8/22], Loss: 1.7506\n","Epoch [2/15], Step [12/22], Loss: 1.7714\n","Epoch [2/15], Step [16/22], Loss: 1.7782\n","Epoch [2/15], Step [20/22], Loss: 1.7629\n","Epoch [3/15], Step [4/22], Loss: 1.7338\n","Epoch [3/15], Step [8/22], Loss: 1.6231\n","Epoch [3/15], Step [12/22], Loss: 1.5430\n","Epoch [3/15], Step [16/22], Loss: 1.4624\n","Epoch [3/15], Step [20/22], Loss: 1.3953\n","Epoch [4/15], Step [4/22], Loss: 1.4455\n","Epoch [4/15], Step [8/22], Loss: 1.3294\n","Epoch [4/15], Step [12/22], Loss: 1.3126\n","Epoch [4/15], Step [16/22], Loss: 1.2976\n","Epoch [4/15], Step [20/22], Loss: 1.2771\n","Epoch [5/15], Step [4/22], Loss: 1.2900\n","Epoch [5/15], Step [8/22], Loss: 1.2243\n","Epoch [5/15], Step [12/22], Loss: 1.2478\n","Epoch [5/15], Step [16/22], Loss: 1.1986\n","Epoch [5/15], Step [20/22], Loss: 1.1926\n","Epoch [6/15], Step [4/22], Loss: 1.1978\n","Epoch [6/15], Step [8/22], Loss: 1.1741\n","Epoch [6/15], Step [12/22], Loss: 1.1554\n","Epoch [6/15], Step [16/22], Loss: 1.1180\n","Epoch [6/15], Step [20/22], Loss: 1.1184\n","Epoch [7/15], Step [4/22], Loss: 1.0921\n","Epoch [7/15], Step [8/22], Loss: 1.1064\n","Epoch [7/15], Step [12/22], Loss: 1.0901\n","Epoch [7/15], Step [16/22], Loss: 1.0521\n","Epoch [7/15], Step [20/22], Loss: 1.0934\n","Epoch [8/15], Step [4/22], Loss: 1.0707\n","Epoch [8/15], Step [8/22], Loss: 1.0437\n","Epoch [8/15], Step [12/22], Loss: 1.0342\n","Epoch [8/15], Step [16/22], Loss: 0.9892\n","Epoch [8/15], Step [20/22], Loss: 1.0416\n","Epoch [9/15], Step [4/22], Loss: 1.0117\n","Epoch [9/15], Step [8/22], Loss: 0.9958\n","Epoch [9/15], Step [12/22], Loss: 0.9860\n","Epoch [9/15], Step [16/22], Loss: 0.9403\n","Epoch [9/15], Step [20/22], Loss: 0.9576\n","Epoch [10/15], Step [4/22], Loss: 0.9958\n","Epoch [10/15], Step [8/22], Loss: 0.9362\n","Epoch [10/15], Step [12/22], Loss: 0.9351\n","Epoch [10/15], Step [16/22], Loss: 0.9125\n","Epoch [10/15], Step [20/22], Loss: 0.9030\n","Epoch [11/15], Step [4/22], Loss: 0.9113\n","Epoch [11/15], Step [8/22], Loss: 0.9146\n","Epoch [11/15], Step [12/22], Loss: 0.8637\n","Epoch [11/15], Step [16/22], Loss: 0.8419\n","Epoch [11/15], Step [20/22], Loss: 0.8613\n","Epoch [12/15], Step [4/22], Loss: 0.8781\n","Epoch [12/15], Step [8/22], Loss: 0.8614\n","Epoch [12/15], Step [12/22], Loss: 0.8019\n","Epoch [12/15], Step [16/22], Loss: 0.8060\n","Epoch [12/15], Step [20/22], Loss: 0.8401\n"]}],"source":["training_loss = []\n","validation_loss = []\n","\n","total_batch = int(len(newsgroups_train.data)/batch_size)\n","    \n","total_batch_val = int(len(newsgroups_test.data)/batch_size)\n"," \n","\n","for epoch in range(num_epochs):\n","    tr_loss = []\n","    model.train()\n","\n","    total_batch = int(len(newsgroups_train.data)/batch_size)\n","    # Loop over all batches\n","    for i in range(total_batch):\n","        batch_x,batch_y = get_batch(data['train'],i,batch_size)\n","\n","        articles = Variable(torch.LongTensor(batch_x))\n","        labels = Variable(torch.LongTensor(batch_y))\n","        \n","        # Forward + Backward + Optimize\n","        optimizer.zero_grad()  # zero the gradient buffer\n","        _, outputs = model(articles)\n","        #print(outputs.shape, labels.shape)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % display_step == 0:\n","            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n","                   %(epoch+1, num_epochs, i+1, len(newsgroups_train.data)//batch_size, loss.data))    \n","    \n","    model.eval()\n","    training_loss.append(sum(tr_loss)/total_batch)\n","\n","    model.eval()\n","    # Loop over all batches\n","    batch_val_loss = []\n","    for i in range(total_batch_val):\n","        \n","        batch_x,batch_y = get_batch(data['test'],i,batch_size)\n","\n","        articles = Variable(torch.LongTensor(batch_x))\n","        labels = Variable(torch.LongTensor(batch_y))\n","\n","        _, y_test_pred = model(articles)\n","        \n","        test_loss = criterion(y_test_pred, labels)\n","        batch_val_loss.append(test_loss.data)\n","\n","    validation_loss.append(sum(batch_val_loss)/total_batch_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPajSIam6yq1"},"outputs":[],"source":["from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3iAbjvIuVVF"},"outputs":[],"source":["folder_name = 'models/lstm/model_2_09_03/'\n","\n","from matplotlib import pyplot as plt\n","training_loss = np.array(training_loss)\n","validation_loss = np.array(validation_loss)\n","\n","\n","fig = plt.figure()\n","plt.plot(training_loss)\n","plt.plot(validation_loss)\n","plt.legend([\"Training Loss\", \"Validation Loss\"])\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.show()\n","fig.savefig(folder_name + 'loss_vs_epoch.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWcQZohc6wp1"},"outputs":[],"source":["total_batch = int(len(newsgroups_test.data)/batch_size)\n","correct = 0\n","total = 0\n","# Loop over all batches\n","for i in range(total_batch):\n","    batch_x,batch_y = get_batch(data['test'],i,batch_size)\n","\n","    articles = Variable(torch.LongTensor(batch_x))\n","    labels = Variable(torch.LongTensor(batch_y))\n","\n","    # Final validation accuracy\n","    total, correct = 0, 0\n","    model.eval()\n","    _, y_test_pred = model(articles)\n","    predicted = torch.argmax(y_test_pred, 1)\n","\n","    total += y_test_pred.size(0)\n","\n","    correct += (predicted == labels).sum()\n","\n","print(f'Accuracy of the model is: {100*correct/total:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7rXeclFhxzv"},"outputs":[],"source":["# Additional information\n","PATH = folder_name + \"model.pt\"\n","LOSS = 0.8401\n","ACCURACY = 78.1\n","\n","# Store model\n","torch.save({\n","            'accuracy': ACCURACY,\n","            'epoch': num_epochs,\n","            'learning_rate': learning_rate,\n","            'batch_size': batch_size,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': LOSS,\n","            }, PATH)\n","\n","# Store model architecture\n","import pickle\n","file_name = folder_name + 'model_architecture.pickle'\n","with open(file_name, 'wb') as f:\n","    pickle.dump(model, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCG8n8Kvz5y9"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"test_LSTM.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
