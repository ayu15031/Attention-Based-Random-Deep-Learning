{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NNDL-Project-Data-Loading.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOu3ueMFdAwrRBgdSPTVBi9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Code for dataset loading"],"metadata":{"id":"Wx2EDqjGrbor"}},{"cell_type":"code","source":["# Make necessary imports\n","random_state = 0\n","import random\n","import numpy as np\n","random.seed(random_state)\n","np.random.seed(random_state)\n","\n","import re\n","import torch\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from sklearn.datasets import fetch_20newsgroups "],"metadata":{"id":"ztBxFfkA2M3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Provide list of categories to consider\n","categories = ['alt.atheism',\n","              'comp.graphics', \n","              'comp.os.ms-windows.misc',\n","              'comp.sys.ibm.pc.hardware',  \n","              'comp.sys.mac.hardware',\n","              'comp.windows.x', \n","              'misc.forsale', \n","              'rec.autos', \n","              'rec.motorcycles', \n","              'rec.sport.baseball', \n","              'rec.sport.hockey', \n","              'sci.crypt', \n","              'sci.electronics', \n","              'sci.med', \n","              'sci.space', \n","              'soc.religion.christian', \n","              'talk.politics.guns', \n","              'talk.politics.mideast', \n","              'talk.politics.misc', \n","              'talk.religion.misc']\n","\n","# Dictionary for merging similar classes together\n","dict_categories = {0: 0,\n","                   1: 1, \n","                   2: 1,\n","                   3: 1,  \n","                   4: 1,\n","                   5: 1,\n","                   6: 2, \n","                   7: 3, \n","                   8: 3, \n","                   9: 3, \n","                   10: 3,\n","                   11: 4, \n","                   12: 4, \n","                   13: 4, \n","                   14: 4,\n","                   15: 5, \n","                   16: 6,\n","                   17: 6, \n","                   18: 6, \n","                   19: 6}\n","\n","# Training subset\n","newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n","# Testing subset\n","newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n","\n","print('Total samples in training data:',len(newsgroups_train.data))\n","print('Total samples in testing data:',len(newsgroups_test.data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646210489223,"user_tz":480,"elapsed":750,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"}},"outputId":"e640a31c-7742-4864-91a4-b6a7ab35ee75","id":"oNyEr7eo2M3Y"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total samples in training data: 11314\n","Total samples in testing data: 7532\n"]}]},{"cell_type":"code","source":["print(\"Unique data targets: \", np.unique(newsgroups_train.target))\n","\n","print(\"Unique data target names: \\n\", newsgroups_train.target_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646210489225,"user_tz":480,"elapsed":17,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"}},"outputId":"70f61c0d-6fc3-4dfd-c591-357fdc732bfa","id":"YUSsuLMS2M3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique data targets:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n","Unique data target names: \n"," ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"]}]},{"cell_type":"code","source":["def clean(text):\n","  \"\"\" Function to clean the text \"\"\"\n","  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","  texter = re.sub(r\"<br />\", \" \", text)\n","  texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n","  texter = re.sub('&#39;', \"\\\"\", texter)\n","  texter = re.sub('\\n', \" \", texter)\n","  texter = re.sub(' u ',\" you \", texter)\n","  texter = re.sub('`',\"\", texter)\n","  texter = re.sub(' +', ' ', texter)\n","  texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n","  texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n","  texter = re.sub('&amp;', 'and', texter)\n","  texter = re.sub('\\r', ' ',texter)\n","  # Remove numbers from string\n","  texter = re.sub(pattern=r\"[+-]?\\d+(?:\\.\\d+)?\", repl=\"\", string=texter, count=0, flags=0)\n","  texter = texter.replace(\"  \", \" \")\n","  clean = re.compile('<.*?>')\n","  texter = texter.encode('ascii', 'ignore').decode('ascii')\n","  texter = re.sub(clean, '', texter)\n","  if texter == \"\":\n","    texter = \"\"\n","  return texter\n","\n","def get_word_2_index(vocab):\n","  \"\"\" Function to get index for each word \"\"\"\n","  word2index = {}\n","  for i, word in enumerate(vocab):\n","    word2index[word.lower()] = i\n","  return word2index\n","\n","def get_vocab_using_bow(newsgroups_train, newsgroups_test):\n","  \"\"\" Function to get vocabulary and indices for dataset \"\"\"\n","  # Build a vocabulary\n","  vocab = Counter()\n","\n","  # Iterate through training samples\n","  for text in newsgroups_train.data:\n","    text = clean(text)\n","    for word in text.split(' '):\n","      vocab[word.lower()]+=1\n","  # Iterate through testing samples\n","  for text in newsgroups_test.data:\n","    text = clean(text)\n","    for word in text.split(' '):\n","      vocab[word.lower()]+=1\n","\n","  word2index = get_word_2_index(vocab)\n","  print(\"Vocabulary size [Bag-of-words]: \", len(vocab))\n","  return vocab, word2index\n","\n","def get_vocab_using_glove(dim):\n","  \"\"\" Function to get vocabulary using GloVe Embeddings\n","  dim can take a value from 50, 100, 200, 300\"\"\"\n","\n","  vocab = {}\n","  with open(\"/content/drive/MyDrive/Colab Notebooks/NNDL/Project/ARDL/glove.6B.{}d.txt\".format(dim), 'r') as f:\n","    for line in f:\n","      values = line.split()\n","      word = values[0]\n","      vector = np.asarray(values[1:], \"float32\")\n","      vocab[word] = vector\n","\n","  print(\"Vocabulary size [GloVe]: \", len(vocab))\n","  return vocab, None\n","\n","def get_batch(df, i, batch_size, vocab, emb, word2index):\n","  \"\"\" Function to convert text into embeddings for a batch of data \n","  emb can take values \"bow\", \"gloveGEN\" and \"gloveLSTM\" correspoding \n","  to Bag of words model, GloVe embeddings (one vector per paragraph) and\n","  GloVe embeddings (a matrix corresponding per paragraph) respectively.\n","  \"\"\"\n","  batches = []\n","  results = []\n","\n","  texts = df.data[i*batch_size : i*batch_size+batch_size]\n","  categories = df.target[i*batch_size : i*batch_size+batch_size]\n","  \n","  for text in texts:\n","    text = clean(text)\n","    if emb == \"bow\":\n","      layer = np.zeros(len(vocab), dtype=float)\n","    elif emb == \"gloveGEN\":\n","      layer = np.zeros(len(vocab[\"a\"]), dtype=float)\n","    elif emb == \"gloveLSTM\":\n","      layer = []\n","\n","    for word in text.split(' '):\n","      # Computation for bag of words model\n","      if emb == \"bow\":\n","        layer[word2index[word.lower()]] += 1\n","      # Computation for GloVe embedding - general\n","      elif emb == \"gloveGEN\":\n","        if word in vocab:\n","          layer += vocab[word]\n","      # Computation for GloVe embedding - LSTM\n","      elif emb == \"gloveLSTM\":\n","        if word in vocab:\n","          layer.append(vocab[word])\n","      else:\n","        print(\"### Invalid embedding type ###\")\n","\n","    batches.append(layer)\n","\n","  for category in categories:\n","    index_y = dict_categories[category]\n","    results.append(index_y)\n","  \n","  return np.array(batches),np.array(results)"],"metadata":{"id":"rDy6sr0a2M3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parameters\n","learning_rate = 0.01\n","num_epochs = 10\n","batch_size = 150\n","display_step = 1\n","\n","# Network Parameters\n","hidden_size = 100      # 1st layer and 2nd layer number of features\n","\n","#####\n","#vocab, word2index = get_vocab_using_bow(newsgroups_train, newsgroups_test)\n","#input_size = len(vocab) # Words in vocab\n","#####\n","vocab, word2index = get_vocab_using_glove(dim=300) # dim = 50, 100, 200, 300\n","input_size = len(vocab[\"a\"]) # Words in vocab\n","#####\n","\n","num_classes = 7         # Categories: graphics, sci.space and baseball\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","class OurNet(nn.Module):\n","     def __init__(self, input_size, hidden_size, num_classes):\n","        super(OurNet, self).__init__()\n","        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n","        self.relu = nn.ReLU()\n","        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n","        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n","\n","     def forward(self, x):\n","        out = self.layer_1(x)\n","        out = self.relu(out)\n","        out = self.layer_2(out)\n","        out = self.relu(out)\n","        out = self.output_layer(out)\n","        return out\n","# input [batch_size, n_labels]\n","# output [max index for each item in batch, ... ,batch_size-1]\n","loss = nn.CrossEntropyLoss()\n","input = Variable(torch.randn(2, 7), requires_grad=True)\n","print(input)\n","target = Variable(torch.LongTensor(2).random_(7))\n","output = loss(input, target)\n","output.backward()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646210907964,"user_tz":480,"elapsed":32876,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"}},"outputId":"691f0ed8-cef7-4acc-99df-ca92e3be56e2","id":"53E_ZMiP2M3g"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size [GloVe]:  400000\n","tensor([[-0.3591, -0.3044, -0.2761,  0.6093, -0.4150, -0.8868, -0.9012],\n","        [ 0.4209, -0.0140,  0.8190,  0.2695, -0.3682, -0.6611,  0.6439]],\n","       requires_grad=True)\n"]}]},{"cell_type":"code","source":["net = OurNet(input_size, hidden_size, num_classes)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()  \n","optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n","#####\n","emb = \"gloveGEN\" #\"bow\", \"gloveGEN\" and \"gloveLSTM\"\n","#####\n","# Train the Model\n","for epoch in range(num_epochs):\n","    total_batch = int(len(newsgroups_train.data)/batch_size)\n","    # Loop over all batches\n","    for i in range(total_batch):\n","        batch_x,batch_y = get_batch(newsgroups_train,i,batch_size,vocab, emb, word2index)\n","        articles = Variable(torch.FloatTensor(batch_x))\n","        labels = Variable(torch.LongTensor(batch_y))\n","        # print(\"articles\",articles)\n","        # print(batch_x, labels)\n","        # print(\"size labels\",labels.size())\n","\n","        # Forward + Backward + Optimize\n","        optimizer.zero_grad()  # zero the gradient buffer\n","        outputs = net(articles)\n","        #print(outputs.shape, labels.shape)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i+1) % 4 == 0:\n","            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n","                   %(epoch+1, num_epochs, i+1, len(newsgroups_train.data)//batch_size, loss.data))\n","\n","\n","        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646210645627,"user_tz":480,"elapsed":83422,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"}},"outputId":"e09a01da-0347-423c-c0db-1bdc0eb06b27","id":"9AJz8gxy2M3j"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Step [4/75], Loss: 35.4270\n","Epoch [1/10], Step [8/75], Loss: 8.3492\n","Epoch [1/10], Step [12/75], Loss: 2.7281\n","Epoch [1/10], Step [16/75], Loss: 2.2970\n","Epoch [1/10], Step [20/75], Loss: 1.7194\n","Epoch [1/10], Step [24/75], Loss: 1.7143\n","Epoch [1/10], Step [28/75], Loss: 1.7290\n","Epoch [1/10], Step [32/75], Loss: 1.7897\n","Epoch [1/10], Step [36/75], Loss: 1.8303\n","Epoch [1/10], Step [40/75], Loss: 1.4828\n","Epoch [1/10], Step [44/75], Loss: 1.5176\n","Epoch [1/10], Step [48/75], Loss: 1.4640\n","Epoch [1/10], Step [52/75], Loss: 1.4455\n","Epoch [1/10], Step [56/75], Loss: 1.2459\n","Epoch [1/10], Step [60/75], Loss: 1.2882\n","Epoch [1/10], Step [64/75], Loss: 1.3257\n","Epoch [1/10], Step [68/75], Loss: 1.2401\n","Epoch [1/10], Step [72/75], Loss: 1.3542\n","Epoch [2/10], Step [4/75], Loss: 1.2921\n","Epoch [2/10], Step [8/75], Loss: 1.2329\n","Epoch [2/10], Step [12/75], Loss: 1.2873\n","Epoch [2/10], Step [16/75], Loss: 1.5232\n","Epoch [2/10], Step [20/75], Loss: 1.2279\n","Epoch [2/10], Step [24/75], Loss: 1.2392\n","Epoch [2/10], Step [28/75], Loss: 1.2413\n","Epoch [2/10], Step [32/75], Loss: 1.4278\n","Epoch [2/10], Step [36/75], Loss: 1.2171\n","Epoch [2/10], Step [40/75], Loss: 1.2079\n","Epoch [2/10], Step [44/75], Loss: 1.4024\n","Epoch [2/10], Step [48/75], Loss: 1.2003\n","Epoch [2/10], Step [52/75], Loss: 1.2148\n","Epoch [2/10], Step [56/75], Loss: 1.0027\n","Epoch [2/10], Step [60/75], Loss: 1.1535\n","Epoch [2/10], Step [64/75], Loss: 1.2939\n","Epoch [2/10], Step [68/75], Loss: 1.1526\n","Epoch [2/10], Step [72/75], Loss: 1.1476\n","Epoch [3/10], Step [4/75], Loss: 1.1740\n","Epoch [3/10], Step [8/75], Loss: 1.1245\n","Epoch [3/10], Step [12/75], Loss: 0.9749\n","Epoch [3/10], Step [16/75], Loss: 1.1397\n","Epoch [3/10], Step [20/75], Loss: 1.0149\n","Epoch [3/10], Step [24/75], Loss: 1.0498\n","Epoch [3/10], Step [28/75], Loss: 0.9849\n","Epoch [3/10], Step [32/75], Loss: 1.1029\n","Epoch [3/10], Step [36/75], Loss: 1.0144\n","Epoch [3/10], Step [40/75], Loss: 1.0547\n","Epoch [3/10], Step [44/75], Loss: 1.3435\n","Epoch [3/10], Step [48/75], Loss: 1.1814\n","Epoch [3/10], Step [52/75], Loss: 1.0911\n","Epoch [3/10], Step [56/75], Loss: 0.9705\n","Epoch [3/10], Step [60/75], Loss: 1.0383\n","Epoch [3/10], Step [64/75], Loss: 1.0731\n","Epoch [3/10], Step [68/75], Loss: 1.0504\n","Epoch [3/10], Step [72/75], Loss: 1.1340\n","Epoch [4/10], Step [4/75], Loss: 1.2432\n","Epoch [4/10], Step [8/75], Loss: 0.9232\n","Epoch [4/10], Step [12/75], Loss: 0.8879\n","Epoch [4/10], Step [16/75], Loss: 1.0352\n","Epoch [4/10], Step [20/75], Loss: 0.9083\n","Epoch [4/10], Step [24/75], Loss: 1.0757\n","Epoch [4/10], Step [28/75], Loss: 1.1072\n","Epoch [4/10], Step [32/75], Loss: 1.2016\n","Epoch [4/10], Step [36/75], Loss: 0.9922\n","Epoch [4/10], Step [40/75], Loss: 1.0117\n","Epoch [4/10], Step [44/75], Loss: 1.5945\n","Epoch [4/10], Step [48/75], Loss: 1.1308\n","Epoch [4/10], Step [52/75], Loss: 1.0991\n","Epoch [4/10], Step [56/75], Loss: 0.9834\n","Epoch [4/10], Step [60/75], Loss: 1.1111\n","Epoch [4/10], Step [64/75], Loss: 1.0423\n","Epoch [4/10], Step [68/75], Loss: 1.0809\n","Epoch [4/10], Step [72/75], Loss: 1.0954\n","Epoch [5/10], Step [4/75], Loss: 1.2470\n","Epoch [5/10], Step [8/75], Loss: 0.9171\n","Epoch [5/10], Step [12/75], Loss: 0.8783\n","Epoch [5/10], Step [16/75], Loss: 1.0322\n","Epoch [5/10], Step [20/75], Loss: 0.9377\n","Epoch [5/10], Step [24/75], Loss: 0.9056\n","Epoch [5/10], Step [28/75], Loss: 0.9645\n","Epoch [5/10], Step [32/75], Loss: 1.1077\n","Epoch [5/10], Step [36/75], Loss: 0.9788\n","Epoch [5/10], Step [40/75], Loss: 1.0165\n","Epoch [5/10], Step [44/75], Loss: 1.1592\n","Epoch [5/10], Step [48/75], Loss: 1.1499\n","Epoch [5/10], Step [52/75], Loss: 1.2375\n","Epoch [5/10], Step [56/75], Loss: 1.0892\n","Epoch [5/10], Step [60/75], Loss: 1.2269\n","Epoch [5/10], Step [64/75], Loss: 1.1200\n","Epoch [5/10], Step [68/75], Loss: 1.1212\n","Epoch [5/10], Step [72/75], Loss: 1.0736\n","Epoch [6/10], Step [4/75], Loss: 1.0773\n","Epoch [6/10], Step [8/75], Loss: 0.9740\n","Epoch [6/10], Step [12/75], Loss: 0.9643\n","Epoch [6/10], Step [16/75], Loss: 1.1558\n","Epoch [6/10], Step [20/75], Loss: 0.9970\n","Epoch [6/10], Step [24/75], Loss: 1.0100\n","Epoch [6/10], Step [28/75], Loss: 0.9378\n","Epoch [6/10], Step [32/75], Loss: 1.0041\n","Epoch [6/10], Step [36/75], Loss: 0.9130\n","Epoch [6/10], Step [40/75], Loss: 0.9911\n","Epoch [6/10], Step [44/75], Loss: 1.2209\n","Epoch [6/10], Step [48/75], Loss: 0.9339\n","Epoch [6/10], Step [52/75], Loss: 1.0325\n","Epoch [6/10], Step [56/75], Loss: 0.8801\n","Epoch [6/10], Step [60/75], Loss: 0.8895\n","Epoch [6/10], Step [64/75], Loss: 0.9699\n","Epoch [6/10], Step [68/75], Loss: 0.9609\n","Epoch [6/10], Step [72/75], Loss: 1.1742\n","Epoch [7/10], Step [4/75], Loss: 1.0406\n","Epoch [7/10], Step [8/75], Loss: 0.8858\n","Epoch [7/10], Step [12/75], Loss: 0.8575\n","Epoch [7/10], Step [16/75], Loss: 1.1011\n","Epoch [7/10], Step [20/75], Loss: 0.9052\n","Epoch [7/10], Step [24/75], Loss: 0.9683\n","Epoch [7/10], Step [28/75], Loss: 0.9889\n","Epoch [7/10], Step [32/75], Loss: 1.0315\n","Epoch [7/10], Step [36/75], Loss: 0.8979\n","Epoch [7/10], Step [40/75], Loss: 1.0136\n","Epoch [7/10], Step [44/75], Loss: 1.1119\n","Epoch [7/10], Step [48/75], Loss: 0.8973\n","Epoch [7/10], Step [52/75], Loss: 1.0369\n","Epoch [7/10], Step [56/75], Loss: 0.8574\n","Epoch [7/10], Step [60/75], Loss: 0.8339\n","Epoch [7/10], Step [64/75], Loss: 0.9453\n","Epoch [7/10], Step [68/75], Loss: 0.9114\n","Epoch [7/10], Step [72/75], Loss: 1.0578\n","Epoch [8/10], Step [4/75], Loss: 1.0888\n","Epoch [8/10], Step [8/75], Loss: 0.9403\n","Epoch [8/10], Step [12/75], Loss: 0.8783\n","Epoch [8/10], Step [16/75], Loss: 1.2225\n","Epoch [8/10], Step [20/75], Loss: 0.9955\n","Epoch [8/10], Step [24/75], Loss: 1.1456\n","Epoch [8/10], Step [28/75], Loss: 1.0054\n","Epoch [8/10], Step [32/75], Loss: 1.0280\n","Epoch [8/10], Step [36/75], Loss: 1.0001\n","Epoch [8/10], Step [40/75], Loss: 1.1295\n","Epoch [8/10], Step [44/75], Loss: 1.1104\n","Epoch [8/10], Step [48/75], Loss: 0.9345\n","Epoch [8/10], Step [52/75], Loss: 0.9985\n","Epoch [8/10], Step [56/75], Loss: 0.8442\n","Epoch [8/10], Step [60/75], Loss: 0.8371\n","Epoch [8/10], Step [64/75], Loss: 0.9205\n","Epoch [8/10], Step [68/75], Loss: 0.9866\n","Epoch [8/10], Step [72/75], Loss: 1.0700\n","Epoch [9/10], Step [4/75], Loss: 1.0938\n","Epoch [9/10], Step [8/75], Loss: 0.8589\n","Epoch [9/10], Step [12/75], Loss: 0.8759\n","Epoch [9/10], Step [16/75], Loss: 0.9639\n","Epoch [9/10], Step [20/75], Loss: 0.8885\n","Epoch [9/10], Step [24/75], Loss: 0.9242\n","Epoch [9/10], Step [28/75], Loss: 1.0083\n","Epoch [9/10], Step [32/75], Loss: 0.9989\n","Epoch [9/10], Step [36/75], Loss: 0.9044\n","Epoch [9/10], Step [40/75], Loss: 1.0528\n","Epoch [9/10], Step [44/75], Loss: 1.2076\n","Epoch [9/10], Step [48/75], Loss: 0.9171\n","Epoch [9/10], Step [52/75], Loss: 1.0027\n","Epoch [9/10], Step [56/75], Loss: 0.8926\n","Epoch [9/10], Step [60/75], Loss: 0.8832\n","Epoch [9/10], Step [64/75], Loss: 0.9110\n","Epoch [9/10], Step [68/75], Loss: 0.9879\n","Epoch [9/10], Step [72/75], Loss: 1.0780\n","Epoch [10/10], Step [4/75], Loss: 1.0505\n","Epoch [10/10], Step [8/75], Loss: 0.8642\n","Epoch [10/10], Step [12/75], Loss: 0.8286\n","Epoch [10/10], Step [16/75], Loss: 0.9325\n","Epoch [10/10], Step [20/75], Loss: 0.8567\n","Epoch [10/10], Step [24/75], Loss: 0.8964\n","Epoch [10/10], Step [28/75], Loss: 0.9635\n","Epoch [10/10], Step [32/75], Loss: 0.9635\n","Epoch [10/10], Step [36/75], Loss: 0.9163\n","Epoch [10/10], Step [40/75], Loss: 0.9909\n","Epoch [10/10], Step [44/75], Loss: 1.0942\n","Epoch [10/10], Step [48/75], Loss: 0.8824\n","Epoch [10/10], Step [52/75], Loss: 1.0195\n","Epoch [10/10], Step [56/75], Loss: 0.8579\n","Epoch [10/10], Step [60/75], Loss: 0.8379\n","Epoch [10/10], Step [64/75], Loss: 0.9514\n","Epoch [10/10], Step [68/75], Loss: 0.9862\n","Epoch [10/10], Step [72/75], Loss: 1.0336\n"]}]},{"cell_type":"code","source":["# Test the Model\n","correct = 0\n","total = 0\n","total_test_data = len(newsgroups_test.target)\n","batch_x_test,batch_y_test = get_batch(newsgroups_test,0,total_test_data, vocab, emb, word2index)\n","articles = Variable(torch.FloatTensor(batch_x_test))\n","labels = torch.LongTensor(batch_y_test)\n","outputs = net(articles)\n","_, predicted = torch.max(outputs.data, 1)\n","total += labels.size(0)\n","correct += (predicted == labels).sum()\n","\n","print('Accuracy of the network on the 1180 test articles: %d %%' % (100 * correct / total))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JbjvoBM9wrv","executionInfo":{"status":"ok","timestamp":1646210650552,"user_tz":480,"elapsed":4978,"user":{"displayName":"SIDDHANT PATIL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCXkU5plXz3uswst5MysEDUW8imlnq40_vPjcy=s64","userId":"17239780046951965775"}},"outputId":"27f31d88-fd92-4a88-b380-1213ae9e62c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network on the 1180 test articles: 63 %\n"]}]}]}